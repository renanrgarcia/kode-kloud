# EKS

- Pre-requisites:
  - AWS account.
  - AWS CLI installed.
  - Kubectl CLI installed.
  - EKS Cluster Role.
  - AWS IAM Authenticator installed for nodegroup.
  - VPC.
  - EC2 Key Pair to access the nodes via SSH.
  - AWS basics.
- Create the EKS cluster:
  - Go to the AWS Console.
  - Select the region: `us-east-1`.
  - Services -> Elastic Kubernetes Service.
  - Create Cluster.
  - Cluster name: `example-voting-app`.
  - Kubernetes version: `1.16`.
  - Role: `eksClusterRole`.
  - VPC: `eksVPC`.
  - Subnets: `eksSubnet1`, `eksSubnet2`, `eksSubnet3`.
  - Create.
- Add a nodegroup:
  - Go to the EKS Cluster -> Configuration -> Compute.
  - Name: `demo-workers`.
  - SSH key pair: `eksKeyPair`.
  - Subnets: `eksSubnet1`, `eksSubnet2`, `eksSubnet3`.
  - Compute configuration: default.
  - Scaling configuration: default.
  - Create.
- Using the AWS CLI:
  - `aws eks --region us-east-1 update-kubeconfig --name example-voting-app`.
    - This command will update the `~/.kube/config` file with the EKS cluster configuration.
  - `kubectl get nodes`.
- We cannot access the master nodes, only the worker nodes. They are managed by AWS.
- Clone the example-voting-app repo.
  - `git clone <repo>`.
- Change the type of service from `NodePort` to `LoadBalancer` in `voting-app-service.yaml` and `result-app-service.yaml`.
  - This is beacause the EKS does not have a built-in load balancer.
- Apply the artifacts:
  - `kubectl apply -f .`
- Check the environment:
  - `kubectl get all`.
- Test the application by opening the external IP of the load balancer in the browser.
